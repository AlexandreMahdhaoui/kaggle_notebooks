{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport time\nimport math\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_probability as tfp\n\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CoordiNET architecture 1\n\n    - Compute Distances through coordinates using the standard euclidian distance equation\n    - Once we have the 729x729x1 distances matrix corresponding to the flatten 9x9x9 matrix, we do simultaneous row/column permutation in order to shuffle the array\n    - We apply the noise before shuffling the 729x729x1 array. Because we reconstruct the noisy matrix and not the original one \n    - In the neural network, we conv the shuffled input and convTranspose it again -> It's an autoencoder, we try to reconstruct the original distance matrix from the shuffled one.\n    \n    - We have to define what kind of noise and transformation we want to add to the distance matrix:\n        - Compute a 729x729x1 matrix of noize. If the noize get above a specified threshold there will be a specified transformation to this cell\n        - Transformation must be pairwise, because we need to get back an Hermitian matrix. FIXED with mirrorDiagonally(mat, size)\n        \nIf there is very high correlated cells, i.e. cells that have low distances between every other cells: we need to be able to create this kind of random cubes in order to simulate a real case\n\n\nTry changing the distribution of the distances with rankGauss for example"},{"metadata":{},"cell_type":"markdown","source":"# What to do next?\n\n    DONE\n    Try using a rank gauss before and after quite every step\n    -> Why ? Because we are not looking to compute the exact value of each cells \n    We are trying to compute the rank values of each cells in order to get an idea of which cell ended up in which other cell\n\nTry creating a neural layer where the network computes row permutations. Our problem is a permutation problem, then we must find the best way to compute those permutations"},{"metadata":{},"cell_type":"markdown","source":"# Cube"},{"metadata":{"trusted":true},"cell_type":"code","source":"SIZE = 9\nBATCH_SIZE= 64\nSTEPS_PER_EPOCH = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Cell():\n    def __init__(self, coord, size):\n        self.size = size\n        self.coordinates = coord\n        self.dist_mat = np.zeros(size**3)\n        self.getDistMat(coord, size)\n\n    def getDistMat(self, coord, size):\n        iter_ = 0\n        for k in range(size):\n            for j in range(size):\n                for i in range(size):\n                    coord_ = np.array((i,j,k))\n                    self.dist_mat[iter_] = np.linalg.norm(coord_ - coord)\n                    iter_ += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Cube():\n    def __init__(self, size):\n        self.size = size\n        self.cells = []\n        self.getCells()\n        self.dist_mat = np.zeros((size**3, size**3))\n        self.getDistMat()\n    \n    def getCells(self):\n        size=self.size\n        for k in range(size):\n            for j in range(size):\n                for i in range(size):\n                    coord = np.array((i,j,k))\n                    self.cells.append(Cell(coord, size))\n                    \n    def getDistMat(self):\n        cells = self.cells\n        for i in range(self.size**3):\n            self.dist_mat[i] = cells[i].dist_mat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset generator\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rankRow(x):\n    from scipy.special import erfinv\n    N = x.shape[0]\n    temp = x.argsort()\n    rank_x = temp.argsort() / N\n    rank_x -= rank_x.mean()\n    rank_x *= 2\n    efi_x = erfinv(rank_x)\n    efi_x -= efi_x.mean()\n    return efi_x\n\ndef rankArray(x):\n    x_ = np.array(x)\n    for i in range(x.shape[0]):\n        x_[i] = rankRow(x_[i])\n    return x_\n\ndef rankBatch(X):\n    X_ = []\n    for x in X:\n        x = np.reshape(x, (SIZE**3, SIZE**3))\n        X_.append(rankArray(x))\n    return X #np.expand_dims(X,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DatasetGenerator():\n    def __init__(self, size, rng_level=0, ds_size=100, batch_size=32):\n        self.cube = Cube(size)\n        self.size = self.cube.size\n        self.rng_level = rng_level\n        self.ds_size = ds_size\n        self.population = np.arange(self.size**3).tolist()\n        self.batch_size = batch_size\n    \n    def getGen(self):\n        ds_size = self.ds_size\n        batch_size = self.batch_size\n        for i in range(ds_size):\n            mat = np.array(self.cube.dist_mat)\n            X,y = [],[]\n            for n in range(batch_size):\n                trans_mat = self.transformMat(mat)\n                permut_mat = self.permutMat(trans_mat)\n                X.append((rankArray(permut_mat)+1.474569062)/(2*1.474569062)), y.append(rankArray(trans_mat))\n            #yields X, y (the shuffled transformed mat and the unshuffled transformed mat)\n            X,y = np.array(X), np.array(y)\n            yield np.expand_dims(X,3), np.expand_dims(y,3)\n    \n    def transformMat(self, mat_):\n        population = self.population\n        # Compute overcorrelated and undercorrelated transformation\n        over_corr = int(self.size**3*random.uniform(0.001,0.05)) # between 0.1% and 5% of the features\n        under_corr = int(self.size**3*random.uniform(0.05,0.3)) # between 5% and 30% of the features\n        idx = random.sample(population, over_corr+under_corr)\n        oc_idx = idx[:over_corr]\n        oc_scalar = 1/(np.random.randint(low=10 ,high=100 ,size=over_corr)) #We divide the over correlated because they have low distances between everything\n        uc_idx = idx[over_corr:]\n        uc_scalar = np.random.randint(low=10 ,high=100 ,size=under_corr)\n        for i in range(over_corr):\n            mat_[oc_idx[i]] = mat_[oc_idx[i]]*oc_scalar[i]\n        for i in range(under_corr):\n            mat_[uc_idx[i]] = mat_[uc_idx[i]]*uc_scalar[i]\n        rand_mat = 1 - (np.random.random((self.size**3,self.size**3))-0.5)*self.rng_level\n        mat_ = mat_*rand_mat\n        return self.mirrorDiagonally(mat_)\n    \n    def mirrorDiagonally(self,mat):\n        for i in range(self.size):\n            for j in range(self.size):\n                mat[i,j] = mat[j,i]\n        return mat\n\n    def permutMat(self, mat):\n        rng_permut = np.random.permutation(self.size**3)\n        mat_ = mat[rng_permut, :]\n        mat_ = mat_[:,rng_permut]\n        return mat_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_gen = DatasetGenerator(SIZE, batch_size=BATCH_SIZE).getGen()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\n\nclass VAE(keras.Model):\n    def __init__(self, ds_gen, latent_dim=2):\n        super(VAE, self).__init__()\n        self.latent_dim = latent_dim\n    \n    \n    def train(self,epochs=100, batch_size=BATCH_SIZE):\n        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n        for epoch in range(1, epochs+1):\n            t = time.time()\n            last_loss=0\n            ds_gen = DatasetGenerator(SIZE, batch_size=BATCH_SIZE).getGen()\n            for X, y in ds_gen:\n                gradients, loss = self.compute_gradients(X,y)\n                self.apply_gradients(optimizer, gradients, self.trainable_variables)\n                last_loss = loss\n            print('Epoch {}, Loss: {}, Time spent: {:.2f}'.format(epoch, last_loss, time.time() - t))\n    \n    def inferenceNet(self,x):\n        x = keras.layers.InputLayer(input_shape=(SIZE**3,SIZE**3,1))(x)\n        x = keras.layers.Flatten()(x)\n        x = keras.layers.Dense(1024, activation='relu')(x)\n        x = keras.layers.Dense(512, activation='relu')(x)\n        x = keras.layers.Dense(self.latent_dim*2)(x)        \n        return x\n    \n    def generativeNet(self,x):\n        z = keras.layers.InputLayer(input_shape=(self.latent_dim))(x)\n        z = keras.layers.Dense(512, activation='relu')(z)\n        z = keras.layers.Dense(1024, activation='relu')(z)\n        z = keras.layers.Dense(SIZE**3 * SIZE**3)(z)\n        z = keras.layers.Reshape(target_shape=(SIZE**3,SIZE**3,1))(z)\n        return z\n    \n    def encode(self,x):\n        mean_logvar = self.inferenceNet(x)\n        N = mean_logvar.shape[0]\n        mean = tf.slice(mean_logvar, [0, 0], [N, self.latent_dim])\n        logvar = tf.slice(mean_logvar, [0, self.latent_dim], [N, self.latent_dim])\n        return mean, logvar\n    \n    def decode(self,z, apply_sigmoid=False):\n        logits = self.generativeNet(z)\n        if apply_sigmoid:\n            probs = tf.sigmoid(logits)\n            return probs\n        return logits\n    \n    def reparameterize(self, mean, logvar):\n        eps = tf.random.normal(shape=mean.shape)\n        return eps*tf.exp(logvar*.5) + mean\n    \n    def compute_gradients(self, x, y):\n        with tf.GradientTape() as tape:\n            loss = self.compute_loss(x, y)\n        return tape.gradient(loss, self.trainable_variables), loss    \n    \n    def compute_loss(self, x,y):\n        mean, logvar = self.encode(x)\n        z = self.reparameterize(mean, logvar)\n        x_logits = self.decode(z)\n        x_logits = tf.cast(rankBatch(x_logits), 'float32')\n        y = tf.cast(y, 'float32')\n        loss = tf.keras.losses.MAE(y, x_logits)\n        loss = tf.convert_to_tensor(np.sum(loss))\n        return loss\n    \n    def apply_gradients(self, optimizer, gradients, variables):\n        optimizer.apply_gradients(zip(gradients, variables))\n        pass\n        \n\nmodel = VAE(ds_gen)\nmodel.train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sources\n\n    - Image restoration: https://research.nvidia.com/sites/default/files/pubs/2017-03_Loss-Functions-for/NN_ImgProc.pdf"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}